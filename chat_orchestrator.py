# -*- coding: utf-8 - *-
"""
Created on Mon Nov  3 16:04:10 2025

@author: geam9
"""

from __future__ import annotations
import os
import re
import json
from typing import Dict, Any, List
from pydantic import BaseModel, Field
import streamlit as st
from dotenv import dotenv_values


config = dotenv_values()


# --------------------------
# ChatOpenAI wrapper (light)
# --------------------------
class ChatOpenAI:
    """
    Wrapper minimalista. Si no hay OPENAI_API_KEY, responde con heurÃ­stica local.
    Reemplaza por tu cliente real (langchain/openai) si lo deseas.
    """

    def __init__(self, model: str = "gpt-4o-mini", temperature: float = 0.4):
        self.model = model
        self.temperature = temperature
        # self.enabled = bool(os.environ.get("OPENAI_API_KEY"))
        self.enabled = bool(st.secrets["OPENAI_API_KEY"])
        # self.enabled = bool(config["OPENAI_API_KEY"])

    def chat(self, messages: List[Dict[str, str]]) -> str:
        if not self.enabled:
            # Fallback simple: eco con tips
            last = messages[-1]["content"] if messages else ""
            return ("(demo sin LLM) Entiendo. A partir de lo que me cuentas, "
                    "priorizarÃ© cursos intro y prÃ¡cticos; si te gusta la creatividad "
                    "y anÃ¡lisis, mezclarÃ© IA bÃ¡sica + marketing digital + proyectos cortos. "
                    f"Mensaje recibido: {last[:200]}...")
        # â†“ Si integras el cliente real, haz la llamada aquÃ­ y retorna el texto
        # Ej. con openai: client.chat.completions.create(model=..., messages=messages)
        # Devuelve `response_text`
        return "(demo) LLM activo â€” agrega tu cliente aquÃ­."


# --------------------------
# Estado de entrevista
# --------------------------
class ProfileState(BaseModel):
    language: str = "es"
    # Campos del catÃ¡logo
    area: str = ""
    level: str = ""
    max_hours: float | None = 40.0
    access: str = ""         # REA/Redireccionamiento/Moodle
    population: str = ""
    keywords_text: str = ""
    # Campos evocadores (psico/vocacionales)
    age: int | None = None
    short_bio: str = ""      # â€œcuÃ©ntame sobre tiâ€
    self_style: str = ""     # â€œÂ¿cÃ³mo te describirÃ­as?â€
    interests: List[str] = Field(default_factory=list)  # hobbies, curiosidades
    # â€œimpacto socialâ€, â€œcreatividadâ€, â€œseguridadâ€, etc.
    values: List[str] = Field(default_factory=list)
    learning_style: str = ""  # â€œprÃ¡cticoâ€, â€œteÃ³ricoâ€, â€œproyectosâ€, â€œmicro-leccionesâ€
    goals: str = ""          # â€œquiero emprenderâ€, â€œbusco empleo rÃ¡pidoâ€, etc.
    constraints: str = ""    # horarios, conectividad, dispositivos
    # Control de conversaciÃ³n
    step: int = 0
    confirmed: bool = False


EVOCATIVE_QUESTIONS_ES = [
    "Â¡Hola! ğŸ˜Š Â¿CÃ³mo estÃ¡s hoy? Â¿QuÃ© edad tienes?",
    "CuÃ©ntame un poco sobre ti: Â¿quÃ© te entusiasma Ãºltimamente?",
    "Â¿CÃ³mo te describirÃ­as en pocas palabras (p. ej., creativo, analÃ­tico, prÃ¡ctico, social)?",
    "Â¿QuÃ© intereses o hobbies tienes (ej.: tecnologÃ­a, diseÃ±o, negocios, ciencia, arte, servicio social)?",
    "Â¿QuÃ© valoras mÃ¡s al aprender: resultados rÃ¡pidos, profundidad teÃ³rica, proyectos, comunidad?",
    "Â¿CÃ³mo te gusta aprender: cursos cortos, retos prÃ¡cticos, lecturas, videos, mentores?",
    "En esta plataforma: Â¿quÃ© esperas lograr en 1â€“3 meses?",
    "Â¿CuÃ¡ntas horas a la semana podrÃ­as dedicar? (nÃºmero aproximado)",
]

FOLLOWUP_ES = [
    "Con lo que me cuentas, Â¿te gustarÃ­a empezar por fundamentos o prefieres saltar directo a cosas aplicadas?",
    "Â¿Te interesan rutas con certificaciÃ³n/constancia o te basta con aprender prÃ¡ctico?",
    "Â¿Hay alguna restricciÃ³n o preferencia tÃ©cnica? (acceso REA/Moodle, conexiÃ³n, dispositivo)",
    "Â¿Te gusta este path inicial? Â¿QuÃ© le cambiarÃ­as o agregarÃ­as?",
]


def extract_number(s: str) -> int | None:
    m = re.search(r"(\d+)", s or "")
    return int(m.group(1)) if m else None


def update_state_from_text(state: ProfileState, user_msg: str) -> ProfileState:
    # HeurÃ­stica simple para llenar slots durante la charla
    if state.age is None:
        age = extract_number(user_msg)
        if age:
            state.age = age
    # keywords
    kws = [w.strip().lower() for w in re.split(
        r"[,\;/]| y | and ", user_msg) if 2 <= len(w.strip()) <= 32]
    # si el usuario menciona cosas tipo "marketing", "datos", etc. sÃºmalas a interests
    interest_hits = [w for w in kws if w in {"datos", "data", "marketing", "diseÃ±o", "programaciÃ³n", "ia", "inteligencia", "excel", "python",
                                             "finanzas", "proyectos", "emprendimiento", "servicio", "social", "salud", "docencia", "seguridad", "ciberseguridad", "cloud"}]
    if interest_hits:
        merged = list(dict.fromkeys((state.interests or []) + interest_hits))
        state.interests = merged[:10]
    # estilo/aprendizaje
    if any(t in user_msg.lower() for t in ["proyecto", "proyectos", "hands-on", "prÃ¡ctic"]):
        state.learning_style = state.learning_style or "proyectos"
    return state


def build_query_from_state(state: ProfileState) -> str:
    # Query semÃ¡ntica hÃ­brida
    parts = []
    if state.area:
        parts.append(f"area:{state.area}")
    if state.level:
        parts.append(f"level:{state.level}")
    if state.access:
        parts.append(f"access:{state.access}")
    if state.population:
        parts.append(f"population:{state.population}")
    if state.keywords_text:
        parts.append(state.keywords_text)
    # intereses/valores
    if state.interests:
        parts.append("intereses: " + ", ".join(state.interests))
    if state.values:
        parts.append("valores: " + ", ".join(state.values))
    if state.learning_style:
        parts.append("aprendizaje:" + state.learning_style)
    if state.goals:
        parts.append("meta:" + state.goals)
    if state.constraints:
        parts.append("restricciones:" + state.constraints)
    return " | ".join(parts) or "fundamentos para principiantes"


def llm_intro_coach(llm: ChatOpenAI, state: ProfileState, user_msg: str) -> str:
    messages = [
        {"role": "system", "content": "Eres un coach vocacional amable y prÃ¡ctico. Haces preguntas cortas, conectas intereses con cursos y justificas sugerencias con claridad. No inventes datos del catÃ¡logo."},
        {"role": "user", "content": f"Idioma: {
            state.language}. Usuario dice: {user_msg}."}
    ]
    return llm.chat(messages)


def llm_explain_track(llm: ChatOpenAI, state: ProfileState, courses: List[dict]) -> str:
    # Genera explicaciÃ³n amigable de por quÃ© ese orden y cÃ³mo encaja con la persona
    bullets = []
    for c in courses[:6]:
        r = c["row"]
        bullets.append(f"- {r.get('Curso', '(sin nombre)')} Â· {r.get(
            'Nivel de complejidad', '')} Â· {r.get('DuraciÃ³n del Curso', '')}")
    plan = "\n".join(bullets)
    messages = [
        {"role": "system", "content": "Eres un asesor que explica rutas de aprendizaje en lenguaje claro, de bÃ¡sico a avanzado, conectando intereses/valores del usuario con los cursos."},
        {"role": "user", "content": f"Perfil: {state.model_dump()}. PropÃ³n un orden (starterâ†’aplicadoâ†’proyecto), 4â€“8 cursos. Lista breve:\n{
            plan}\nExplica cÃ³mo encaja con sus intereses/estilo. Cierra preguntando si desea cambios."}
    ]
    return llm.chat(messages)
